from transformers import pipeline, AutoTokenizer

# –ø—É—Ç—å –∫ –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏
pt = r"models/RUPunct_big"

# –∑–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∏ –º–æ–¥–µ–ª—å
tk = AutoTokenizer.from_pretrained(pt, strip_accents=False, add_prefix_space=True)
classifier = pipeline(
    "ner",
    model=pt,
    tokenizer=tk,
    aggregation_strategy="first",
    device=-1  # CPU
)


def process_token(token, label):
    mapping = {
        "LOWER_O": token,
        "LOWER_PERIOD": token + ".",
        "LOWER_COMMA": token + ",",
        "LOWER_QUESTION": token + "?",
        "LOWER_TIRE": token + "‚Äî",
        "LOWER_DVOETOCHIE": token + ":",
        "LOWER_VOSKL": token + "!",
        "LOWER_PERIODCOMMA": token + ";",
        "LOWER_DEFIS": token + "-",
        "LOWER_MNOGOTOCHIE": token + "...",
        "LOWER_QUESTIONVOSKL": token + "?!",
        "UPPER_O": token.capitalize(),
        "UPPER_PERIOD": token.capitalize() + ".",
        "UPPER_COMMA": token.capitalize() + ",",
        "UPPER_QUESTION": token.capitalize() + "?",
        "UPPER_TIRE": token.capitalize() + " ‚Äî",
        "UPPER_DVOETOCHIE": token.capitalize() + ":",
        "UPPER_VOSKL": token.capitalize() + "!",
        "UPPER_PERIODCOMMA": token.capitalize() + ";",
        "UPPER_DEFIS": token.capitalize() + "-",
        "UPPER_MNOGOTOCHIE": token.capitalize() + "...",
        "UPPER_QUESTIONVOSKL": token.capitalize() + "?!",
        "UPPER_TOTAL_O": token.upper(),
        "UPPER_TOTAL_PERIOD": token.upper() + ".",
        "UPPER_TOTAL_COMMA": token.upper() + ",",
        "UPPER_TOTAL_QUESTION": token.upper() + "?",
        "UPPER_TOTAL_TIRE": token.upper() + " ‚Äî",
        "UPPER_TOTAL_DVOETOCHIE": token.upper() + ":",
        "UPPER_TOTAL_VOSKL": token.upper() + "!",
        "UPPER_TOTAL_PERIODCOMMA": token.upper() + ";",
        "UPPER_TOTAL_DEFIS": token.upper() + "-",
        "UPPER_TOTAL_MNOGOTOCHIE": token.upper() + "...",
        "UPPER_TOTAL_QUESTIONVOSKL": token.upper() + "?!",
    }
    return mapping.get(label, token)


def split_text(text, chunk_size=200):
    """–†–∞–∑–¥–µ–ª—è–µ—Ç –¥–ª–∏–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞—Å—Ç–∏ –ø–æ chunk_size —Å–ª–æ–≤."""
    words = text.split()
    for i in range(0, len(words), chunk_size):
        yield " ".join(words[i:i + chunk_size])


# --- –ß–∏—Ç–∞–µ–º –≤—Ö–æ–¥–Ω–æ–π —Ñ–∞–π–ª ---
with open("transcript.txt", "r", encoding="utf-8") as f:
    input_text = f.read().strip()

print("‚úÖ –¢–µ–∫—Å—Ç –∑–∞–≥—Ä—É–∂–µ–Ω, –¥–ª–∏–Ω–∞:", len(input_text), "—Å–∏–º–≤–æ–ª–æ–≤")

output = ""

# --- –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫—É—Å–∫–∞–º–∏ ---
for i, chunk in enumerate(split_text(input_text)):
    print(f"\nüîπ –û–±—Ä–∞–±–æ—Ç–∫–∞ –±–ª–æ–∫–∞ {i+1}...")
    preds = classifier(chunk)
    if not preds:
        print("‚ö†Ô∏è –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –Ω–µ—Ç –¥–ª—è —ç—Ç–æ–≥–æ –±–ª–æ–∫–∞.")
        continue
    for item in preds:
        output += " " + process_token(item["word"].strip(), item["entity_group"])

# --- –í—ã–≤–æ–¥ ---
if output.strip():
    print("\n‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç:")
    print(output.strip()[:500] + "..." if len(output) > 500 else output.strip())

    with open("output.txt", "w", encoding="utf-8") as f:
        f.write(output.strip())
    print("\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤ output.txt")
else:
    print("‚ö†Ô∏è –ú–æ–¥–µ–ª—å –Ω–µ –≤–µ—Ä–Ω—É–ª–∞ —Ç–µ–∫—Å—Ç. –ü—Ä–æ–≤–µ—Ä—å –º–æ–¥–µ–ª—å –∏–ª–∏ –ø—É—Ç—å –∫ –Ω–µ–π.")
